## Non-parameter testing
https://www.zhihu.com/column/p/49472487

## MS ExP
[Ron Kohavi KDD 2015 Talk](https://www.youtube.com/watch?v=ZfhQ-fIg4EU&feature=youtu.be&t=2m59s)

- Bing ads
- Bing search

Good resource: [Exp-platform](https://exp-platform.com/2018StrataABtutorial/)

## [Optimizely KDD 2017 Paper Talk](https://www.youtube.com/watch?v=AJX4W3MwKzU)
- Sequential Testing

## [Multi-arm/Contextual bandit](https://multithreaded.stitchfix.com/blog/2018/11/08/bandits/)

## Cases 
#### Uber
- https://eng.uber.com/xp/
- https://eng.uber.com/analyzing-experiment-outcomes

#### Airbnb
Dynamic P, cluster-based, variance reduction
- https://medium.com/airbnb-engineering/experiments-at-airbnb-e2db3abf39e7 
- https://medium.com/airbnb-engineering/experimentation-measurement-for-search-engine-optimization-b64136629760
- https://youtu.be/rxQ6D-QQMWc?t=728

Dynamic p value: how long to stop experiment
- Plot dyas in experiment vs pvalue and treatment effect
- Calculate sample size
- Running simulations(with varying values for parameters like the real effect size, variance and different levels of certainty) and deriving a curve that gives us a dynamic (in time) p-value threshold to determine whether or not an early result is worth investigating


Another lesson: breakdown by browers
- redesign search page but turns out basically a null effect
- segment by browser: most performe fine, except IE, he new design broke an important click-through action for certain older versions of IE

It is good to be scientific about your relationship with the reporting system. I
- f something doesn’t seem right or if it seems too good to be true, investigate it. A simple way of doing this is to run dummy experiments, but any knowledge about how the system behaves is useful for interpreting results.

[Selection bias](https://medium.com/airbnb-engineering/selection-bias-in-online-experimentation-c3d67795cceb)

#### Patreon (small company)

https://patreonhq.com/please-please-dont-a-b-test-that-980a9630e4fb (公开了内部的Experiment Template)

https://patreonhq.com/thats-not-a-hypothesis-25666b01d5b4

#### [Inscart](https://tech.instacart.com/it-all-depends-4bb7b22e854b)
- In their logistics system, we cannot split samples either by customer or by shopper since they are all interdependent.

Some ways:
- Simulation: we “replayed” the history of customer and shopper behaviors with the existing algorithm. It's mising lots of other info like feedback loop, capacity and cnanot meausre guardrail metrics directly
- Before and after: it is not clear how much of the effect was caused by exogenous factors such as weather conditions.
- DID: SF vs Oakland: Oakland is far from being a perfect representation of SF. There is an endless list of other factors which might be at play such as traffic conditions, customer promotions, system changes, etc.


They split samples by zone and day
- ttest
- linear regression(equivalent to ttest)
- multivariate: isolate other factors we care about zoen, DoW, week number

By controlling for other variables, multivariate regression dramatically reduces the standard error of the estimate and hence the p-value and false negatives


[How long: run a power analysis to determine the sample size]
(https://docs.google.com/presentation/d/1mxKfEEcVZjup8tCNiYw0g6aeZ8T1G6Sr74mE2Xe7Wtg/edit#slide=id.g4ed02fc4e4_0_78)
Examine variability of the effect estimate via sampling distributions(each distribution is generated by running A/A 500 times)

#### Netflix
https://medium.com/netflix-techblog/its-all-a-bout-testing-the-netflix-experimentation-platform-4e1ca458c15 

During design, one approach we utilize is running repeated randomizations, i.e. **‘re-randomization’**. In particular, we keep randomizing until we find a randomization that gives us the maximum desired level of balance on key variables across test cells. This approach generally enables us to define more similar test groups (i.e. getting closer to apples to apples comparison).
- we can only simultaneously balance on a limited number of observed variables, and it is very difficult to find identical geographic units on all dimensions, and
- we can still face noisy results with large confidence intervals due to small sample size. 

Method1:  Difference in differences (diff-in-diff or DID) comparison is a very common approach used in quasi experiments. 

In diff-in-diff, we usually consider two time periods; pre and post intervention. We utilize the pre-intervention period to generate baselines for our metrics, and normalize post intervention values by the baseline. This normalization is a simple but very powerful way of controlling for inherent differences between treatment and control groups. For example, let’s say our success metric is signups and we are running a quasi experiment in France. We have Paris and Lyon in two test cells. We cannot directly compare signups in two cities as populations are very different. Normalizing with respect to pre-intervention signups would reduce variation and help us make comparisons at the same scale.

Method2: Success Metrics With Historical Observations But Small Sample Size
- High variation in outcome metrics combined with small sample size can be a problem to design a well powered experiment using traditional diff-in-diff like approaches.
- We turn the intervention (e.g. advertising) “on” and “off” repeatedly over time in different patterns and geographic units to capture short term effects.
- To estimate the treatment effect, we fit a dynamic linear model (aka DLM), a type of state space model where the observations are conditionally Gaussian.

Method3: Success Metrics Without Historical Observations
- We sometimes face cases where we don’t have success metrics with historical observations. For example, Netflix promotes its new shows that are yet to be launched on service to increase member engagement once the show is available.
- We do this by using relevant pre-treatment proxies, e.g. viewing of similar shows, interest in Netflix originals or similar genres. 
- We have observed that controlling for geographic as well as individual level differences work best in minimizing confounding effects and improving precision.           
  - For example, if members in Toronto watch more Netflix originals than members in other cities in Canada, we should then control for pre-treatment Netflix originals viewing at both individual and city level to capture within and between unit variation separately.

## When should we A/B testing:
- Established products: UI, Function
- Fledgling products

## Analyze
- [HTE](http://www.unofficialgoogledatascience.com/2019/04/misadventures-in-experiments-for-growth.html)
- [Percent increase](http://jwegan.com/growth-hacking/wrong-way-to-analyze-experiments/)
- [Udacity Case](https://github.com/shubhamlal11/Udacity-AB-Testing-Final-Project)

## Consideration
- Long term goal: https://research.google/pubs/pub43887/


## Network Effect
- [Linkedin graph clustering method](https://engineering.linkedin.com/blog/2019/06/detecting-interference--an-a-b-test-of-a-b-tests)
  - Cluster the LinkedIn graph into 10,000 clusters. The graph comprises all active LinkedIn members as nodes and their “connections” as edges.  
  - Split these clusters into two parallel experiments: individual vs cluster based
- Doordash: time switchback 
- Inscart: zone and day
